{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3cd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e70411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_layer_size = 784 #fixed size of the input 28 x 28 = 748\n",
    "Out_layer_size = 10 #fixed size of the output layer 10 digits of MNIST output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a96d16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # This is for hidden layer of neuron size 10\n",
    "        # If you want to increase your layer size, change 10 in both\n",
    "        self.fc1 = nn.Linear(Input_layer_size, 10)  # Input layer to Hidden layer\n",
    "        # If you want another layer, add one between these two\n",
    "        self.fc2 = nn.Linear(10, Out_layer_size)   # Hidden layer to Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        #If you add more layers, you need to change the following logic\n",
    "        #relu is for hidden layers\n",
    "        #softmax is the classifier applied only to the output layer\n",
    "        x = x.view(-1, 784)\n",
    "            \n",
    "        #print(\"Input to fc1:\", x)  # Print the input to the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        print(\"Output of fc1 (before ReLU):\", x)  # Output of fc1 before applying ReLU\n",
    "        x = F.relu(x)\n",
    "        print(\"Output of fc1 (after ReLU):\", x)  # Output of fc1 after applying ReLU\n",
    "        x = self.fc2(x)\n",
    "        print(\"Output of fc2 (before LogSoftmax):\", x)  # Output of fc2 before applying LogSoftmax\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        print(\"Output of fc2 (after LogSoftmax):\", x)  # Output of fc2 after applying LogSoftmax\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323cdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device, model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5929f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_reshape_weights(filename, num_neurons, input_size):\n",
    "    # Load entire weights from binary file\n",
    "    all_weights = np.fromfile(filename, dtype=np.float32)  # Adjust dtype according to actual data type in C++\n",
    "\n",
    "    # Create an array to hold the selected weights\n",
    "    selected_weights = np.zeros(num_neurons * input_size, dtype=np.float32)\n",
    "\n",
    "    # Populate selected_weights with the appropriate values from all_weights\n",
    "    index = 0\n",
    "    for i in range(num_neurons):\n",
    "        for j in range(input_size):\n",
    "            selected_weights[index] = all_weights[i * input_size + j]\n",
    "            index += 1\n",
    "\n",
    "    # Reshape the selected_weights array if needed, e.g., into a matrix\n",
    "    reshaped_weights = selected_weights.reshape(num_neurons, input_size)\n",
    "    return reshaped_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af810eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_biases_from_file(filename):\n",
    "    try:\n",
    "        # Load the entire file content into a NumPy array\n",
    "        biases = np.fromfile(filename, dtype=np.float32)\n",
    "        return biases\n",
    "    except IOError as e:\n",
    "        print(f\"Failed to open or read from file: {filename}\")\n",
    "        print(e)\n",
    "        return np.array([])  # Return an empty NumPy array in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b44900d-ecac-4ecf-9e9b-526c14cb9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(first_image, cmap='gray')\n",
    "# plt.title(f'Label: {first_label}')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # Step 2: Load your BMP image\n",
    "# image_path = 'mnist_first_image.bmp'  # Update this to the path of your BMP file\n",
    "# image1 = Image.open(image_path)#.convert('L')  # Convert image to grayscale if it's not already\n",
    "\n",
    "# image_path = 'final_image_scaled.bmp'  # Update this to the path of your BMP file\n",
    "# image2 = Image.open(image_path)#.convert('L')  # Convert image to grayscale if it's not already\n",
    "\n",
    "# image1 = transform(image1)\n",
    "# image2 = transform(image2)\n",
    "\n",
    "\n",
    "# # Calculate differences\n",
    "# difference = image1 - image2\n",
    "\n",
    "# # Find non-zero differences\n",
    "# different_indices = torch.nonzero(difference, as_tuple=False)\n",
    "\n",
    "# # Print the shape of the images and the differences\n",
    "# print(image1.shape)\n",
    "# print(image2.shape)\n",
    "# #print(difference)\n",
    "\n",
    "# # Print indices where the images differ\n",
    "# print(different_indices.shape)\n",
    "\n",
    "# image = image1\n",
    "\n",
    "\n",
    "# # Step 4: Add a batch dimension\n",
    "# image = image.unsqueeze(0)\n",
    "# print(image.shape)\n",
    "\n",
    "# # Assuming your model is named 'model' and is already loaded and trained\n",
    "# # and 'device' is defined as in your code\n",
    "\n",
    "# # Move image to the same device as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e750044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the dataset loading to not use DataLoader since we want to access a single image directly\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "#Dataset loading\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "first_image, first_label = test_dataset[0]\n",
    "\n",
    "\n",
    "# # Undo normalization for saving the image in a visually accurate form\n",
    "# image_for_saving = first_image * 0.3081 + 0.1307\n",
    "# image_for_saving = image_for_saving.squeeze()  # Remove the channel dimension for grayscale image\n",
    "\n",
    "# # Convert the tensor to a PIL Image\n",
    "# image_for_saving = image_for_saving.numpy()  # Convert tensor to numpy array\n",
    "# image_for_saving = (image_for_saving * 255).astype('uint8')  # Scale to 0-255 for image saving\n",
    "# pil_image = Image.fromarray(image_for_saving)\n",
    "\n",
    "# # Save the image as BMP\n",
    "# pil_image.save('first_image_mnist.bmp')\n",
    "\n",
    "# # Undo normalization if any: assuming the mean and std were 0.1307 and 0.3081 respectively\n",
    "# first_image = first_image * 0.3081 + 0.1307\n",
    "# first_image = first_image.squeeze()  # Remove channel dimension if it exists (for 1-channel image)\n",
    "\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "# first_image = first_image.numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(first_image.shape)\n",
    "\n",
    "# first_image = transform(first_image)\n",
    "\n",
    "# print(first_image.shape)\n",
    "\n",
    "# # for i in range(first_image.shape[1]):\n",
    "# #     for j in range(first_image.shape[2]):\n",
    "# #         first_image[i][0][j] = 1\n",
    "\n",
    "# #first_image.fill_(1)\n",
    "\n",
    "# for i in range(first_image.shape[1]):\n",
    "#     str_prt = ''\n",
    "#     for j in range(first_image.shape[2]):\n",
    "#         str_prt += str(first_image[i][0][j]) + \" \"\n",
    "#     print(str_prt)\n",
    "\n",
    "\n",
    "# Load the saved BMP image\n",
    "#image_path = 'first_image_mnist.bmp'\n",
    "image_path = 'final_image_scaled.bmp'\n",
    "saved_image = Image.open(image_path).convert('L')  # Ensure it's loaded in grayscale mode\n",
    "\n",
    "first_image = transform(saved_image)\n",
    "\n",
    "# #first_image.fill_(1)\n",
    "\n",
    "image = first_image.unsqueeze(0)\n",
    "\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91c19e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "(10,)\n",
      "(10, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# Assuming only you have one hidden layer\n",
    "filename = 'fc1_weight.bin'\n",
    "num_neurons = 10  # adjust as per your model specifics\n",
    "input_size = 784  # total input size\n",
    "weights_fc1 = load_and_reshape_weights(filename, num_neurons, input_size)\n",
    "# for i in range(weights_fc1.shape[0]):\n",
    "#     for j in range(weights_fc1.shape[1]):\n",
    "#         weights_fc1[i][j] = 1\n",
    "print(weights_fc1.shape)\n",
    "\n",
    "\n",
    "filename = 'fc1_bias.bin'\n",
    "biases_fc1 = load_biases_from_file(filename)\n",
    "# for i in range(biases_fc1.shape[0]):\n",
    "#     biases_fc1[i] = 0\n",
    "print(biases_fc1.shape)\n",
    "\n",
    "\n",
    "filename = 'fc2_weight.bin'\n",
    "num_neurons = 10  # adjust as per your model specifics\n",
    "input_size = 10  # total input size\n",
    "weights_fc2 = load_and_reshape_weights(filename, num_neurons, input_size)\n",
    "print(weights_fc2.shape)\n",
    "\n",
    "\n",
    "filename = 'fc2_bias.bin'\n",
    "biases_fc2 = load_biases_from_file(filename)\n",
    "print(biases_fc2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0934a902",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_fc1 = torch.from_numpy(weights_fc1)\n",
    "biases_fc1 = torch.from_numpy(biases_fc1)\n",
    "weights_fc2 = torch.from_numpy(weights_fc2)\n",
    "biases_fc2 = torch.from_numpy(biases_fc2)\n",
    "\n",
    "# Assign these tensors to the model's parameters\n",
    "model.fc1.weight.data = weights_fc1\n",
    "model.fc1.bias.data = biases_fc1\n",
    "model.fc2.weight.data = weights_fc2\n",
    "model.fc2.bias.data = biases_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f396fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Output of fc1 (before ReLU): tensor([[ 4.7999, -3.9072,  1.2859,  0.5208,  7.8938, -3.4408, -0.7152,  9.4483,\n",
      "          2.0219,  8.9399]])\n",
      "Output of fc1 (after ReLU): tensor([[4.7999, 0.0000, 1.2859, 0.5208, 7.8938, 0.0000, 0.0000, 9.4483, 2.0219,\n",
      "         8.9399]])\n",
      "Output of fc2 (before LogSoftmax): tensor([[ -0.5250,  -8.5922,   5.8019,   5.4075,  -4.1025,  -0.0232, -12.0365,\n",
      "          11.4665,  -0.7413,   2.6801]])\n",
      "Output of fc2 (after LogSoftmax): tensor([[-1.1997e+01, -2.0065e+01, -5.6706e+00, -6.0650e+00, -1.5575e+01,\n",
      "         -1.1496e+01, -2.3509e+01, -5.9600e-03, -1.2214e+01, -8.7923e+00]])\n",
      "tensor([[7]])\n"
     ]
    }
   ],
   "source": [
    "# Perform the classification\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(image.dim())\n",
    "# print(image[0].dim())\n",
    "# for i in range(768):\n",
    "#     image[0,0,0,i] = 1\n",
    "with torch.no_grad():  # No need to track gradients for validation/testing\n",
    "    output = model(image)\n",
    "    #print(output)\n",
    "    predicted_label = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413532de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c5d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
