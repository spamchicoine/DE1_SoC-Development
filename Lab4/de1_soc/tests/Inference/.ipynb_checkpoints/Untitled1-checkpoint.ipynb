{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f1774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db522e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 10)  # Input layer to Hidden layer\n",
    "        self.fc2 = nn.Linear(10, 10)   # Hidden layer to Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x.view(-1, 784)))\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc438548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c28c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5982ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c176a6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.366400\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.027204\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.452948\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.479945\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.416718\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.273356\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.316451\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.456952\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.215560\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.421111\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.325584\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.290585\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.407773\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.303997\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.308628\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.271613\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.258890\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.123242\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.315844\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.164300\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.219179\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.228068\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.375660\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.317392\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.248641\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.236115\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.424942\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.378141\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.580838\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.187307\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.274148\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.308247\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.184552\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.131827\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.103656\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.189141\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.241339\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.257526\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.263285\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.279017\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.206132\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.326317\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.229840\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.185396\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.311198\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.173908\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.212397\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.216319\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.250701\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.148009\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.420606\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.139627\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.240230\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.078652\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.163711\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.317577\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.277888\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.181222\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.172460\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.075210\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.335100\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.146010\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.185018\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.193180\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.221914\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.411291\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.292844\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.182393\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.285438\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.272461\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.194389\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.267266\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.188219\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.213032\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.437467\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.303604\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.219113\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.254094\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.216813\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.193539\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.367209\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.289493\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.210172\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.331508\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.108482\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.245190\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.140810\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.198213\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.573329\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.129266\n"
     ]
    }
   ],
   "source": [
    "# Setup device, model, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, 10):  # Adjust the number of epochs as needed\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    #test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a34ef22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to save model weights and biases as binary files\n",
    "def save_weights_and_biases(model):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        #parameter.data.cpu().numpy()\n",
    "        param_data = parameter.data.cpu().numpy().flatten()  # Move the tensor to CPU and convert to NumPy\n",
    "        file_name = f\"{name.replace('.', '_')}.bin\"  # Replace dots in names with underscores for filenames\n",
    "        param_data.tofile(file_name)\n",
    "        print(f\"Saved {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cc71b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fc1_weight.bin\n",
      "Saved fc1_bias.bin\n",
      "Saved fc2_weight.bin\n",
      "Saved fc2_bias.bin\n"
     ]
    }
   ],
   "source": [
    "save_weights_and_biases(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "68304f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 5, Actual Label: 5\n"
     ]
    }
   ],
   "source": [
    "# Modify the dataset loading to not use DataLoader since we want to access a single image directly\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Access the first image and its label\n",
    "image, label = test_dataset[0]\n",
    "\n",
    "# The image is already in tensor form, but we need to add a batch dimension at the beginning\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "# Assuming your model is named 'model' and is already loaded and trained\n",
    "# and 'device' is defined as in your code\n",
    "\n",
    "# Move image to the same device as the model\n",
    "image = image.to(device)\n",
    "\n",
    "# Perform the classification\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to track gradients for validation/testing\n",
    "    output = model(image)\n",
    "    predicted_label = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label.item()}, Actual Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6c4eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1762,  4.4226,  0.6379,  6.9450,  8.1929,  3.7955,  0.8442,  1.4294,\n",
      "         -4.7221,  9.6552]])\n"
     ]
    }
   ],
   "source": [
    "def print_layer_output(module, input, output):\n",
    "    print(output)\n",
    "\n",
    "# Assuming 'model' is your instance of Net\n",
    "handle = model.fc1.register_forward_hook(print_layer_output)\n",
    "\n",
    "# Now, when you perform a forward pass, the output of fc1 will be printed:\n",
    "# For example, using the first image from the MNIST dataset as shown in the previous code snippet.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(image)\n",
    "\n",
    "# Don't forget to remove the hook when you're done to avoid altering the behavior unintentionally\n",
    "handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "118f0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in model.named_parameters():\n",
    "    if name == 'fc1.weight':\n",
    "        param_data = parameter.data.cpu().numpy()  # Move the tensor to CPU and convert to NumPy\n",
    "        print(param_data.shape)\n",
    "    if name == 'fc1.bias':\n",
    "        param_bias = parameter.data.cpu().numpy()  # Move the tensor to CPU and convert to NumPy\n",
    "        print(param_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76df7480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "996cf6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "flattened_tensor = image.view(-1)\n",
    "\n",
    "print(flattened_tensor.size())  # Should print torch.Size([784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a1470a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23367587  0.29422194  0.02999098 -0.07587945  0.01631195  0.15406579\n",
      "  0.35718498  0.05255867  0.26541394 -0.24861467]\n"
     ]
    }
   ],
   "source": [
    "# Convert tensor to numpy array\n",
    "flattened_array = flattened_tensor.numpy()\n",
    "\n",
    "initial_index = 9*16\n",
    "final_index = 10*16\n",
    "# Perform matrix multiplication\n",
    "result = param_data.flatten() @ flattened_array[:28]\n",
    "#result = param_data[:,initial_index:final_index] @ flattened_array[initial_index:final_index] #+ param_bias\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "54928924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00917219, -0.01012454,  0.00575211, -0.03331763, -0.00510417,\n",
       "        -0.03795844, -0.01233142, -0.10264165, -0.04232003, -0.05728726,\n",
       "        -0.0186581 ,  0.00743315, -0.0058446 ,  0.01992852,  0.05828208,\n",
       "         0.07711641],\n",
       "       [-0.04916483, -0.03351561, -0.06335033, -0.01391312,  0.00356992,\n",
       "        -0.01903719, -0.02954159, -0.05425746, -0.09002895, -0.02478559,\n",
       "        -0.04109292, -0.02898013,  0.02363174,  0.03888603,  0.02864696,\n",
       "        -0.00698481],\n",
       "       [ 0.02705055,  0.01471456,  0.0073227 , -0.00647504,  0.01133923,\n",
       "         0.0080873 , -0.00570219, -0.04784397, -0.03196152, -0.0430212 ,\n",
       "         0.01950537, -0.01283287,  0.02113139, -0.01420124,  0.00347038,\n",
       "        -0.01650197],\n",
       "       [-0.00240677,  0.04210202,  0.04077978,  0.01077317,  0.04211942,\n",
       "         0.02006197,  0.03079542,  0.02601452,  0.06147995,  0.05401466,\n",
       "         0.00661837,  0.01165761,  0.06208581, -0.01045334, -0.00507818,\n",
       "        -0.00781497],\n",
       "       [-0.00411734, -0.02221525, -0.03119496, -0.03249041, -0.01560659,\n",
       "        -0.03964201, -0.00456797,  0.0095369 ,  0.02833061, -0.02964049,\n",
       "        -0.00507176, -0.07464684, -0.01172772,  0.05100407, -0.06047016,\n",
       "        -0.02805744],\n",
       "       [-0.01359947, -0.02046632, -0.02748471,  0.02345512, -0.012042  ,\n",
       "        -0.05504937, -0.01886847,  0.05553001,  0.09640141,  0.10241371,\n",
       "         0.09200335,  0.12061629,  0.05559146,  0.06509211,  0.04371318,\n",
       "         0.0620384 ],\n",
       "       [-0.02390472, -0.05559223, -0.05218099, -0.0534673 , -0.02701674,\n",
       "        -0.01692185, -0.02094617, -0.01030972, -0.03626532,  0.02922871,\n",
       "         0.00470789,  0.04881476,  0.0582055 ,  0.08776173,  0.03853294,\n",
       "         0.04221523],\n",
       "       [ 0.01694713,  0.03050511,  0.07985458,  0.07909759,  0.03460429,\n",
       "         0.04249404,  0.06307586,  0.08638957,  0.03583725,  0.07033967,\n",
       "         0.03753311,  0.04868242,  0.11353103,  0.04324087,  0.05357575,\n",
       "         0.06887189],\n",
       "       [ 0.00207323, -0.0767918 , -0.07573893, -0.05036257, -0.0199293 ,\n",
       "        -0.03489421,  0.03927675,  0.05173036,  0.05216265,  0.07334673,\n",
       "         0.03413327,  0.05725557,  0.01234837,  0.07581176,  0.07757222,\n",
       "         0.06077221],\n",
       "       [ 0.01037249,  0.01348972,  0.03817722, -0.05558083, -0.03274473,\n",
       "        -0.03776494, -0.11717629, -0.0856107 , -0.06223562, -0.04328156,\n",
       "        -0.04677049, -0.03829919, -0.08939771, -0.14278053, -0.06909032,\n",
       "        -0.06160171]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_data[:,initial_index:final_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "226f350f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42421296, -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
       "       -0.42421296, -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
       "       -0.42421296, -0.42421296, -0.42421296, -0.42421296, -0.42421296,\n",
       "       -0.42421296], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_array[initial_index:final_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ac7db8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0458191 ,  0.01698243,  0.01067879, -0.03650709, -0.04565034,\n",
       "        0.00868056, -0.01806587, -0.00648912, -0.00593405,  0.02437885,\n",
       "       -0.0340474 ,  0.01143396, -0.02999866, -0.03985174, -0.01715232,\n",
       "       -0.03415582,  0.000643  , -0.03140581, -0.04144616,  0.02499945,\n",
       "       -0.00829009, -0.01180074, -0.01798108, -0.02656857,  0.00809063,\n",
       "        0.00032076, -0.04492859,  0.00624798,  0.0193358 , -0.03380025,\n",
       "       -0.009534  ,  0.00291923, -0.0245493 , -0.01703419, -0.0416005 ,\n",
       "        0.00504714,  0.00399562,  0.01194636,  0.02461998, -0.02956784,\n",
       "       -0.01487653,  0.02863905,  0.00414832,  0.01333327, -0.01529653,\n",
       "        0.01579291,  0.02066944, -0.01047433,  0.02820223, -0.0325526 ,\n",
       "        0.0129239 , -0.03062511, -0.03449157, -0.01063934,  0.01013319,\n",
       "        0.00465741, -0.01517251,  0.01529183, -0.00821066, -0.04391089,\n",
       "        0.01755815,  0.0156984 ,  0.02072206,  0.0350907 , -0.00157402,\n",
       "        0.01737464,  0.0420107 ,  0.02197047,  0.01803929,  0.04281915,\n",
       "        0.04119278,  0.06407156,  0.00383969,  0.03066   ,  0.00156276,\n",
       "        0.03037463,  0.05053415,  0.03680525,  0.00268937, -0.00752649,\n",
       "        0.01910888, -0.0346001 , -0.0326669 ,  0.01715543, -0.0023356 ,\n",
       "       -0.04487255, -0.03271918,  0.00106715, -0.03017358, -0.03774867,\n",
       "        0.03062229,  0.01064964, -0.00104131,  0.02381248,  0.01022087,\n",
       "        0.02062627,  0.01418199,  0.01914773,  0.0020417 ,  0.02724204,\n",
       "       -0.02214508, -0.05381474,  0.00048483,  0.03882394,  0.01211092,\n",
       "        0.00197446, -0.00215483,  0.05649215, -0.03497107, -0.01340599,\n",
       "        0.02607391,  0.00552917, -0.0137424 ,  0.01061235, -0.03149772,\n",
       "        0.00109027,  0.01349262,  0.03134217, -0.00364642,  0.00470028,\n",
       "       -0.01725285,  0.01069585, -0.00446818, -0.05682241,  0.00286321,\n",
       "       -0.01869762, -0.02839001, -0.07320992,  0.01662451,  0.01190819,\n",
       "       -0.04744968,  0.0078737 ,  0.01370544, -0.00579752,  0.06932711,\n",
       "        0.01184407, -0.01999789,  0.01157891,  0.01715584, -0.01477151],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_data.flatten()[:140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
